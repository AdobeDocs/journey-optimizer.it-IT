---
solution: Journey Optimizer
product: Journey Optimizer
title: Modelli di ottimizzazione automatica
description: Ulteriori informazioni sui modelli di ottimizzazione automatica
badge: label="Legacy" type="Informative"
feature: Ranking, Decision Management
role: User
level: Experienced
exl-id: a85de6a9-ece2-43da-8789-e4f8b0e4a0e7
version: Journey Orchestration
source-git-commit: 3fa90fa707b562ecf2160ec980520bc8bc267a21
workflow-type: tm+mt
source-wordcount: '1492'
ht-degree: 0%

---

# Modelli di ottimizzazione automatica {#auto-optimization-model}

Un modello di ottimizzazione automatica mira a fornire offerte che massimizzano il rendimento (KPI) impostato dai clienti aziendali. Questi KPI potrebbero essere sotto forma di tassi di conversione, ricavi, ecc. A questo punto, l‚Äôottimizzazione automatica si concentra sull‚Äôottimizzazione dei clic dell‚Äôofferta con la conversione dell‚Äôofferta come obiettivo. L‚Äôottimizzazione automatica non √® personalizzata e viene ottimizzata in base alle prestazioni &quot;globali&quot; delle offerte.

## Requisiti del set di dati

Per addestrare un modello di ottimizzazione automatica, il set di dati deve soddisfare i seguenti requisiti minimi:

* Almeno 2 offerte nel set di dati devono avere almeno 100 eventi di visualizzazione e 5 eventi di clic negli ultimi 14 giorni.
* Le offerte con meno di 100 visualizzazioni e/o 5 eventi di clic negli ultimi 14 giorni verranno trattate dal modello come nuove offerte e saranno idonee solo per essere servite dal team di esplorazione.
* Le offerte con pi√π di 100 display e 5 eventi di clic negli ultimi 14 giorni verranno trattate dal modello come offerte esistenti e potranno essere servite da banditi sia di esplorazione che di sfruttamento.

Fino alla prima volta che viene addestrato un modello di ottimizzazione automatica, le offerte all‚Äôinterno di una strategia di selezione che utilizza un modello di ottimizzazione automatica verranno servite a caso.

## Limitazioni {#limitations}

L‚Äôutilizzo di modelli di ottimizzazione automatica per la gestione delle decisioni √® soggetto alle limitazioni seguenti:

* I modelli di ottimizzazione automatica non funzionano con l‚ÄôAPI Batch Decisioning.
* Il feedback necessario per creare il modello deve essere inviato come evento esperienza. Non deve essere inviato automaticamente in [!DNL Journey Optimizer] canali.

## Terminologia {#terminology}

I seguenti termini sono utili quando si parla di ottimizzazione automatica:

* **Slot machine**: un approccio all&#39;ottimizzazione [Slot machine](https://en.wikipedia.org/wiki/Multi-armed_bandit){target="_blank"} compensa l&#39;apprendimento esplorativo e il suo sfruttamento.

* **Campionamento di Thompson**: il campionamento di Thompson √® un algoritmo per i problemi decisionali online in cui le azioni vengono eseguite in sequenza in modo da bilanciare lo sfruttamento di ci√≤ che √® noto per massimizzare le prestazioni immediate e l&#39;investimento di nuove informazioni che possono migliorare le prestazioni future. [Ulteriori informazioni](#thompson-sampling)

* [**Distribuzione Beta**](https://en.wikipedia.org/wiki/Beta_distribution){target="_blank"}: set di [distribuzioni di probabilit√†](https://en.wikipedia.org/wiki/Probability_distribution){target="_blank"} continue definite nell&#39;intervallo [0, 1] [con parametri](https://en.wikipedia.org/wiki/Statistical_parameter){target="_blank"} da due [parametri forma positivi](https://en.wikipedia.org/wiki/Shape_parameter){target="_blank"}.

## Campionamento di Thompson {#thompson-sampling}

L&#39;algoritmo alla base dell&#39;ottimizzazione automatica √® **campionamento di Thompson**. In questa sezione viene descritta l‚Äôintuizione alla base del campionamento di Thompson.

[Il campionamento di Thompson](https://en.wikipedia.org/wiki/Thompson_sampling){target="_blank"}, o banditi bayesiani, √® un approccio bayesiano al problema della slot machine.  L&#39;idea di base consiste nel trattare la ricompensa media ùõç di ogni offerta come una **variabile casuale** e utilizzare i dati raccolti finora per aggiornare la nostra &quot;convinzione&quot; sulla ricompensa media. Questa &quot;convinzione&quot; √® rappresentata matematicamente da una **distribuzione di probabilit√† posteriore** - essenzialmente un intervallo di valori per la ricompensa media, insieme alla plausibilit√† (o probabilit√†) che la ricompensa abbia quel valore per ogni offerta.‚ÄØQuindi, per ogni decisione, **preleveremo un punto da ciascuna di queste distribuzioni di premi posteriori** e selezioneremo l&#39;offerta con la ricompensa campionata con il valore pi√π alto.

Questo processo √® illustrato nella figura seguente, dove sono disponibili 3 diverse offerte. Inizialmente non abbiamo alcuna prova dai dati e supponiamo che tutte le offerte abbiano una distribuzione di ricompensa a posteriori uniforme. Prendiamo un campione dalla distribuzione di ogni offerta di premi a posteriori. Il campione selezionato dalla distribuzione di Offerta 2 ha il valore pi√π alto. Questo √® un esempio di **esplorazione**. Dopo aver mostrato l&#39;Offerta 2, raccogliamo qualsiasi potenziale ricompensa (ad esempio conversione/non conversione) e aggiorniamo la distribuzione posteriore dell&#39;Offerta 2 utilizzando il Teorema di Bayes come spiegato di seguito.  Continuiamo questo processo e aggiorniamo le distribuzioni posteriori ogni volta che viene mostrata un‚Äôofferta e viene raccolto il premio. Nella seconda figura, viene selezionata l&#39;Offerta 3 - nonostante l&#39;Offerta 1 abbia la pi√π alta ricompensa media (la sua distribuzione di ricompensa posteriore √® pi√π a destra), il processo di campionamento da ogni distribuzione ci ha portato a scegliere un&#39;Offerta 3 apparentemente non ottimale. In questo modo, offriamo a noi stessi l&#39;opportunit√† di conoscere meglio la vera distribuzione delle ricompense offerta 3.

Man mano che vengono raccolti pi√π campioni, l‚Äôaffidabilit√† aumenta e si ottiene una stima pi√π accurata del possibile premio (corrispondente a distribuzioni pi√π ridotte).‚ÄØQuesto processo di aggiornamento delle nostre convinzioni man mano che diventano disponibili ulteriori prove √® noto come **Inferenza bayesiana**.

Alla fine, se un&#39;offerta (ad es. Offerta 1) √® un chiaro vincitore, la sua distribuzione successiva della ricompensa sar√† separata dagli altri. A questo punto, per ogni decisione, la ricompensa campionata dall‚ÄôOfferta 1 √® probabilmente la pi√π alta e la sceglieremo con una probabilit√† pi√π elevata. Si tratta di **sfruttamento**. Siamo convinti che l&#39;offerta 1 sia la migliore e quindi viene scelta per massimizzare i premi.

![](../assets/ai-ranking-thompson-sampling.png)

**Figura 1**: *Per ogni decisione, campioniamo un punto dalle distribuzioni di premi posteriori. Verr√† scelta l‚Äôofferta con il valore di esempio pi√π alto (tasso di conversione). Nella fase iniziale, tutte le offerte hanno una distribuzione uniforme, in quanto non disponiamo di alcuna evidenza sui tassi di conversione delle offerte dai dati. Mentre raccogliamo pi√π campioni, le distribuzioni posteriori diventano pi√π strette e pi√π precise. In ultima analisi, l&#39;offerta con il tasso di conversione pi√π alto verr√† scelta ogni volta.*

<!--
![](../assets/ai-ranking-thompson-sampling-initial.png)
![](../assets/ai-ranking-thompson-sampling-intermediate.png)
![](../assets/ai-ranking-thompson-sampling-ultimate.png)
-->

+++**Dettagli tecnici**

Per calcolare/aggiornare le distribuzioni, si utilizza **Teorema di Bayes**. Per ogni offerta ***i***, vogliamo calcolare la loro ***P(ùõçi | dati)***, ovvero per ogni offerta ***i***, quanto √® probabile un valore di ricompensa **ùõçi**, dati i dati raccolti finora per tale offerta.

Dal Teorema Di Bayes:

***Posteriore = Probabilit√† * Precedente***

La **probabilit√† precedente** √® la stima iniziale della probabilit√† di produzione di un output. La probabilit√†, dopo aver raccolto alcune prove, √® nota come **probabilit√† posteriore**.‚ÄØ

L‚Äôottimizzazione automatica √® progettata per prendere in considerazione i premi binari (clic/nessun clic). In questo caso, la probabilit√† rappresenta il numero di successi da N prove ed √® modellata da una **distribuzione binomiale**. Per alcune funzioni di verosimiglianza, se si sceglie una certa distribuzione a priori, la distribuzione a posteriori finisce per essere nella stessa distribuzione della distribuzione a priori. Tale priore √® chiamato **priore coniugato**. Questo tipo di distribuzione a priori rende il calcolo della distribuzione a posteriori molto semplice. La **distribuzione Beta** √® coniugata prima della probabilit√† binomiale (premi binari), quindi √® una scelta comoda e ragionevole per le distribuzioni di probabilit√† precedenti e posteriori. La distribuzione Beta prende due parametri, ***Œ±*** e ***Œ≤***.‚ÄØQuesti parametri possono essere considerati come il conteggio dei successi e degli errori e il valore medio dato da:

![](../assets/ai-ranking-beta-distribution.png)

La funzione Likelihood, come spiegato in precedenza, √® modellata da una distribuzione binomiale, con s successi (conversioni) e f errori (nessuna conversione) e q √® una [variabile casuale](https://en.wikipedia.org/wiki/Random_variable){target="_blank"} con [distribuzione beta](https://en.wikipedia.org/wiki/Beta_distribution){target="_blank"}.

La distribuzione a priori √® modellata dalla distribuzione Beta e la distribuzione a posteriori assume la seguente forma:

![](../assets/ai-ranking-posterior-distribution.svg)

La parte posteriore viene calcolata semplicemente aggiungendo il numero di successi ed errori ai parametri esistenti ***Œ±***, ***Œ≤***.

Per l&#39;ottimizzazione automatica, come mostrato nell&#39;esempio precedente, si inizia con una distribuzione precedente ***Beta(1, 1)*** (distribuzione uniforme) per tutte le offerte e dopo aver ottenuto s successi e f errori per una determinata offerta, la distribuzione posteriore diventa una distribuzione Beta con parametri ***(s+Œ±, f+Œ≤)*** per tale offerta.
+++

**Argomenti correlati**:

Per un approfondimento sul campionamento di Thompson, leggi i seguenti articoli di ricerca:

* [Valutazione empirica del campionamento di Thompson](https://proceedings.neurips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf){target="_blank"}
* [Analisi del campionamento di Thompson per il problema di slot machine](https://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf){target="_blank"}

## Problema di avviamento a freddo {#cold-start}

Il problema di &quot;avviamento a freddo&quot; si verifica quando una nuova offerta viene aggiunta a una campagna e non sono disponibili dati sul tasso di conversione della nuova offerta. Durante questo periodo, dovremo elaborare una strategia che indichi la frequenza con cui questa nuova offerta viene scelta in modo da ridurre al minimo il calo delle prestazioni, mentre raccoglieremo informazioni sul tasso di conversione di questa nuova offerta. Sono disponibili diverse soluzioni per affrontare questo problema. La chiave √® trovare un equilibrio tra l&#39;esplorazione di questa nuova offerta mentre non sacrifichiamo molto lo sfruttamento. Attualmente utilizziamo la &quot;distribuzione uniforme&quot; come stima iniziale del tasso di conversione della nuova offerta (distribuzione precedente). In sostanza, tutti i valori del tasso di conversione hanno la stessa probabilit√† di verificarsi.


![](../assets/ai-ranking-cold-start-strategies.png)

**Figura 2**: *Considera una campagna con 3 offerte. Durante la pubblicazione della campagna, alla campagna viene aggiunta l‚ÄôOfferta 4. Inizialmente non disponiamo di dati sul tasso di conversione dell‚ÄôOfferta 4 e dobbiamo affrontare il problema dell‚Äôavviamento a freddo. Utilizziamo la distribuzione uniforme come ipotesi iniziale sul tasso di conversione dell‚Äôofferta 4, mentre raccogliamo i dati per questa nuova offerta. Come spiegato nella sezione [Campionamento di Thompson](#thompson-sampling), per scegliere quale offerta verr√† mostrata a un utente, campioniamo punti dalle distribuzioni di premi posteriori delle offerte e selezioniamo l&#39;offerta con il valore di esempio pi√π alto. Nell&#39;esempio precedente, l&#39;Offerta 4 viene scelta e successivamente in base alla ricompensa raccolta, la distribuzione posteriore di questa offerta viene aggiornata come spiegato nella sezione [Campionamento di Thompson](#thompson-sampling).*

## Misurazione dell‚Äôincremento {#lift}

&quot;Incremento&quot; √® la metrica utilizzata per misurare le prestazioni di qualsiasi strategia implementata nel servizio di classificazione, rispetto alla strategia di base (serving delle offerte solo in modo casuale).

Ad esempio, se siamo interessati a misurare le prestazioni di una strategia di campionamento di Thompson (TS) utilizzata nel servizio di classificazione e il KPI √® il tasso di conversione (CVR), l‚Äô&quot;incremento&quot; della strategia TS rispetto alla strategia di base √® definito come segue:

![](../assets/ai-ranking-lift.png)
