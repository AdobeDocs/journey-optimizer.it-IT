---
product: experience platform
solution: Experience Platform
title: Modelli di ottimizzazione automatica
description: Ulteriori informazioni sui modelli di ottimizzazione automatica
feature: Ranking, Decision Management
role: User
level: Experienced
exl-id: 8a8b66cb-dd96-4373-bbe0-a67e0dc0b2c0
version: Journey Orchestration
source-git-commit: 0b94bfeaf694e8eaf0dd85e3c67ee97bd9b56294
workflow-type: tm+mt
source-wordcount: '1500'
ht-degree: 0%

---

# Modelli di ottimizzazione automatica {#auto-optimization-model}

Un modello di ottimizzazione automatica mira a fornire offerte che massimizzano il rendimento (KPI) impostato dai clienti aziendali. Questi KPI potrebbero essere sotto forma di tassi di conversione, ricavi, ecc. A questo punto, lâ€™ottimizzazione automatica si concentra sullâ€™ottimizzazione dei clic dellâ€™offerta con la conversione dellâ€™offerta come obiettivo. Lâ€™ottimizzazione automatica non Ã¨ personalizzata e viene ottimizzata in base alle prestazioni &quot;globali&quot; delle offerte.

## Requisiti del set di dati

Per addestrare un modello di ottimizzazione automatica, il set di dati deve soddisfare i seguenti requisiti minimi:

* Almeno 2 offerte nel set di dati devono avere almeno 100 eventi di visualizzazione e 5 eventi di clic negli ultimi 14 giorni.
* Le offerte con meno di 100 visualizzazioni e/o 5 eventi di clic negli ultimi 14 giorni verranno trattate dal modello come nuove offerte e saranno idonee solo per essere servite dal team di esplorazione.
* Le offerte con piÃ¹ di 100 display e 5 eventi di clic negli ultimi 14 giorni verranno trattate dal modello come offerte esistenti e potranno essere servite da banditi sia di esplorazione che di sfruttamento.

Fino alla prima volta che viene addestrato un modello di ottimizzazione automatica, le offerte allâ€™interno di una strategia di selezione che utilizza un modello di ottimizzazione automatica verranno servite a caso.

## Limitazioni {#limitations}

Lâ€™utilizzo di modelli di ottimizzazione automatica per la gestione delle decisioni Ã¨ soggetto alle limitazioni seguenti:

<!--* Auto-optimization models do not work with the Batch Decisioning API.-->
* Il feedback necessario per creare il modello deve essere inviato come evento esperienza. Non deve essere inviato automaticamente in [!DNL Journey Optimizer] canali.

## Terminologia {#terminology}

I seguenti termini sono utili quando si parla di ottimizzazione automatica:

* **Slot machine**: un approccio all&#39;ottimizzazione [Slot machine](https://en.wikipedia.org/wiki/Multi-armed_bandit){target="_blank"} compensa l&#39;apprendimento esplorativo e il suo sfruttamento.

* **Campionamento di Thompson**: il campionamento di Thompson Ã¨ un algoritmo per i problemi decisionali online in cui le azioni vengono eseguite in sequenza in modo da bilanciare lo sfruttamento di ciÃ² che Ã¨ noto per massimizzare le prestazioni immediate e l&#39;investimento di nuove informazioni che possono migliorare le prestazioni future. [Ulteriori informazioni](#thompson-sampling)

* [**Distribuzione Beta**](https://en.wikipedia.org/wiki/Beta_distribution){target="_blank"}: set di [distribuzioni di probabilitÃ ](https://en.wikipedia.org/wiki/Probability_distribution){target="_blank"} continue definite nell&#39;intervallo [0, 1] [con parametri](https://en.wikipedia.org/wiki/Statistical_parameter){target="_blank"} da due [parametri forma positivi](https://en.wikipedia.org/wiki/Shape_parameter){target="_blank"}.

## Campionamento di Thompson {#thompson-sampling}

L&#39;algoritmo alla base dell&#39;ottimizzazione automatica Ã¨ **campionamento di Thompson**. In questa sezione viene descritta lâ€™intuizione alla base del campionamento di Thompson.

[Il campionamento di Thompson](https://en.wikipedia.org/wiki/Thompson_sampling){target="_blank"}, o banditi bayesiani, Ã¨ un approccio bayesiano al problema della slot machine.  L&#39;idea di base consiste nel trattare la ricompensa media ð› di ogni offerta come una **variabile casuale** e utilizzare i dati raccolti finora per aggiornare la nostra &quot;convinzione&quot; sulla ricompensa media. Questa &quot;convinzione&quot; Ã¨ rappresentata matematicamente da una **distribuzione di probabilitÃ  posteriore** - essenzialmente un intervallo di valori per la ricompensa media, insieme alla plausibilitÃ  (o probabilitÃ ) che la ricompensa abbia quel valore per ogni offerta.â€¯Quindi, per ogni decisione, **preleveremo un punto da ciascuna di queste distribuzioni di premi posteriori** e selezioneremo l&#39;offerta con la ricompensa campionata con il valore piÃ¹ alto.

Questo processo Ã¨ illustrato nella figura seguente, dove sono disponibili 3 diverse offerte. Inizialmente non abbiamo alcuna prova dai dati e supponiamo che tutte le offerte abbiano una distribuzione di ricompensa a posteriori uniforme. Prendiamo un campione dalla distribuzione di ogni offerta di premi a posteriori. Il campione selezionato dalla distribuzione di Offerta 2 ha il valore piÃ¹ alto. Questo Ã¨ un esempio di **esplorazione**. Dopo aver mostrato l&#39;Offerta 2, raccogliamo qualsiasi potenziale ricompensa (ad esempio conversione/non conversione) e aggiorniamo la distribuzione posteriore dell&#39;Offerta 2 utilizzando il Teorema di Bayes come spiegato di seguito.  Continuiamo questo processo e aggiorniamo le distribuzioni posteriori ogni volta che viene mostrata unâ€™offerta e viene raccolto il premio. Nella seconda figura, viene selezionata l&#39;Offerta 3 - nonostante l&#39;Offerta 1 abbia la piÃ¹ alta ricompensa media (la sua distribuzione di ricompensa posteriore Ã¨ piÃ¹ a destra), il processo di campionamento da ogni distribuzione ci ha portato a scegliere un&#39;Offerta 3 apparentemente non ottimale. In questo modo, offriamo a noi stessi l&#39;opportunitÃ  di conoscere meglio la vera distribuzione delle ricompense offerta 3.

Man mano che vengono raccolti piÃ¹ campioni, lâ€™affidabilitÃ  aumenta e si ottiene una stima piÃ¹ accurata del possibile premio (corrispondente a distribuzioni piÃ¹ ridotte).â€¯Questo processo di aggiornamento delle nostre convinzioni man mano che diventano disponibili ulteriori prove Ã¨ noto come **Inferenza bayesiana**.

Alla fine, se un&#39;offerta (ad es. Offerta 1) Ã¨ un chiaro vincitore, la sua distribuzione successiva della ricompensa sarÃ  separata dagli altri. A questo punto, per ogni decisione, la ricompensa campionata dallâ€™Offerta 1 Ã¨ probabilmente la piÃ¹ alta e la sceglieremo con una probabilitÃ  piÃ¹ elevata. Si tratta di **sfruttamento**. Siamo convinti che l&#39;offerta 1 sia la migliore e quindi viene scelta per massimizzare i premi.

![](../assets/ai-ranking-thompson-sampling.png)

**Figura 1**: *Per ogni decisione, campioniamo un punto dalle distribuzioni di premi posteriori. VerrÃ  scelta lâ€™offerta con il valore di esempio piÃ¹ alto (tasso di conversione). Nella fase iniziale, tutte le offerte hanno una distribuzione uniforme, in quanto non disponiamo di alcuna evidenza sui tassi di conversione delle offerte dai dati. Mentre raccogliamo piÃ¹ campioni, le distribuzioni posteriori diventano piÃ¹ strette e piÃ¹ precise. In ultima analisi, l&#39;offerta con il tasso di conversione piÃ¹ alto verrÃ  scelta ogni volta.*

+++**Dettagli tecnici**

Per calcolare/aggiornare le distribuzioni, si utilizza **Teorema di Bayes**. Per ogni offerta ***i***, vogliamo calcolare la loro ***P(ð›i | dati)***, ovvero per ogni offerta ***i***, quanto Ã¨ probabile un valore di ricompensa **ð›i**, dati i dati raccolti finora per tale offerta.

Dal Teorema Di Bayes:

***Posteriore = ProbabilitÃ  * Precedente***

La **probabilitÃ  precedente** Ã¨ la stima iniziale della probabilitÃ  di produzione di un output. La probabilitÃ , dopo aver raccolto alcune prove, Ã¨ nota come **probabilitÃ  posteriore**.â€¯

Lâ€™ottimizzazione automatica Ã¨ progettata per prendere in considerazione i premi binari (clic/nessun clic). In questo caso, la probabilitÃ  rappresenta il numero di successi da N prove ed Ã¨ modellata da una **distribuzione binomiale**. Per alcune funzioni di verosimiglianza, se si sceglie una certa distribuzione a priori, la distribuzione a posteriori finisce per essere nella stessa distribuzione della distribuzione a priori. Tale priore Ã¨ chiamato **priore coniugato**. Questo tipo di distribuzione a priori rende il calcolo della distribuzione a posteriori molto semplice. La **distribuzione Beta** Ã¨ coniugata prima della probabilitÃ  binomiale (premi binari), quindi Ã¨ una scelta comoda e ragionevole per le distribuzioni di probabilitÃ  precedenti e posteriori. La distribuzione Beta prende due parametri, ***Î±*** e ***Î²***.â€¯Questi parametri possono essere considerati come il conteggio dei successi e degli errori e il valore medio dato da:

![](../assets/ai-ranking-beta-distribution.png)

La funzione Likelihood, come spiegato in precedenza, Ã¨ modellata da una distribuzione binomiale, con s successi (conversioni) e f errori (nessuna conversione) e q Ã¨ una [variabile casuale](https://en.wikipedia.org/wiki/Random_variable){target="_blank"} con [distribuzione beta](https://en.wikipedia.org/wiki/Beta_distribution){target="_blank"}.

La distribuzione a priori Ã¨ modellata dalla distribuzione Beta e la distribuzione a posteriori assume la seguente forma:

![](../assets/ai-ranking-posterior-distribution.svg)

La parte posteriore viene calcolata semplicemente aggiungendo il numero di successi ed errori ai parametri esistenti ***Î±***, ***Î²***.

Per l&#39;ottimizzazione automatica, come mostrato nell&#39;esempio precedente, si inizia con una distribuzione precedente ***Beta(1, 1)*** (distribuzione uniforme) per tutte le offerte e dopo aver ottenuto s successi e f errori per una determinata offerta, la distribuzione posteriore diventa una distribuzione Beta con parametri ***(s+Î±, f+Î²)*** per tale offerta.
+++

**Argomenti correlati**:

Per un approfondimento sul campionamento di Thompson, leggi i seguenti articoli di ricerca:

* [Valutazione empirica del campionamento di Thompson](https://proceedings.neurips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf){target="_blank"}
* [Analisi del campionamento di Thompson per il problema di slot machine](https://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf){target="_blank"}

## Problema di avviamento a freddo {#cold-start}

Il problema di &quot;avviamento a freddo&quot; si verifica quando una nuova offerta viene aggiunta a una campagna e non sono disponibili dati sul tasso di conversione della nuova offerta. Durante questo periodo, dovremo elaborare una strategia che indichi la frequenza con cui questa nuova offerta viene scelta in modo da ridurre al minimo il calo delle prestazioni, mentre raccoglieremo informazioni sul tasso di conversione di questa nuova offerta. Sono disponibili diverse soluzioni per affrontare questo problema. La chiave Ã¨ trovare un equilibrio tra l&#39;esplorazione di questa nuova offerta mentre non sacrifichiamo molto lo sfruttamento. Attualmente utilizziamo la &quot;distribuzione uniforme&quot; come stima iniziale del tasso di conversione della nuova offerta (distribuzione precedente). In sostanza, tutti i valori del tasso di conversione hanno la stessa probabilitÃ  di verificarsi.

![](../assets/ai-ranking-cold-start-strategies.png)

**Figura 2**: *Considera una campagna con 3 offerte. Durante la pubblicazione della campagna, alla campagna viene aggiunta lâ€™Offerta 4. Inizialmente non disponiamo di dati sul tasso di conversione dellâ€™Offerta 4 e dobbiamo affrontare il problema dellâ€™avviamento a freddo. Utilizziamo la distribuzione uniforme come ipotesi iniziale sul tasso di conversione dellâ€™offerta 4, mentre raccogliamo i dati per questa nuova offerta. Come spiegato nella sezione [Campionamento di Thompson](#thompson-sampling), per scegliere quale offerta verrÃ  mostrata a un utente, campioniamo punti dalle distribuzioni di premi posteriori delle offerte e selezioniamo l&#39;offerta con il valore di esempio piÃ¹ alto. Nell&#39;esempio precedente, l&#39;Offerta 4 viene scelta e successivamente in base alla ricompensa raccolta, la distribuzione posteriore di questa offerta viene aggiornata come spiegato nella sezione [Campionamento di Thompson](#thompson-sampling).*

## Misurazione dellâ€™incremento {#lift}

&quot;Incremento&quot; Ã¨ la metrica utilizzata per misurare le prestazioni di qualsiasi strategia implementata nel servizio di classificazione, rispetto alla strategia di base (serving delle offerte solo in modo casuale).

Ad esempio, se siamo interessati a misurare le prestazioni di una strategia di campionamento di Thompson (TS) utilizzata nel servizio di classificazione e il KPI Ã¨ il tasso di conversione (CVR), lâ€™&quot;incremento&quot; della strategia TS rispetto alla strategia di base Ã¨ definito come segue:

![](../assets/ai-ranking-lift.png)

>[!NOTE]
>
>Attualmente il rapporto Lift Measurement Ã¨ disponibile solo per il modello di intelligenza artificiale [Personalized optimization](personalized-optimization-model.md). [Ulteriori informazioni sul reporting delle decisioni](../../reports/campaign-global-report-cja-code.md#decisioning-reporting)
